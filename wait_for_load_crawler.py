from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException, NoSuchElementException
import time
import json
import os

def wait_for_page_load(driver, max_wait_time=60):
    """ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    print("ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½...")
    
    start_time = time.time()
    last_text_length = 0
    stable_count = 0
    
    while time.time() - start_time < max_wait_time:
        try:
            # æ£€æŸ¥æ˜¯å¦è¿˜æœ‰loadingå…ƒç´ 
            loading_elements = driver.find_elements(By.ID, "preload")
            
            if len(loading_elements) == 0:
                print("âœ… Loadingå…ƒç´ å·²æ¶ˆå¤±ï¼Œé¡µé¢åŠ è½½å®Œæˆï¼")
                return True
            
            # è·å–å½“å‰é¡µé¢æ–‡æœ¬
            body_text = driver.find_element(By.TAG_NAME, "body").text
            current_text_length = len(body_text)
            
            # æ£€æŸ¥å†…å®¹æ˜¯å¦ç¨³å®š
            if current_text_length == last_text_length and current_text_length > 100:
                stable_count += 1
                if stable_count >= 3:  # è¿ç»­3æ¬¡å†…å®¹ç¨³å®š
                    print(f"âœ… é¡µé¢å†…å®¹å·²ç¨³å®š {stable_count} æ¬¡ï¼ŒåŠ è½½å®Œæˆï¼")
                    return True
            else:
                stable_count = 0
            
            last_text_length = current_text_length
            
            # æ£€æŸ¥æ˜¯å¦æœ‰å®é™…å†…å®¹ï¼ˆä¸ä»…ä»…æ˜¯loadingæ–‡æœ¬ï¼‰
            if current_text_length > 500 and "loading" not in body_text.lower():
                print(f"âœ… æ£€æµ‹åˆ°ä¸°å¯Œå†…å®¹ ({current_text_length} å­—ç¬¦)ï¼ŒåŠ è½½å®Œæˆï¼")
                return True
            
            print(f"â³ ä»åœ¨åŠ è½½ä¸­... (å·²ç­‰å¾… {int(time.time() - start_time)}ç§’, æ–‡æœ¬é•¿åº¦: {current_text_length})")
            time.sleep(5)  # å¢åŠ ç­‰å¾…é—´éš”
            
        except Exception as e:
            print(f"ç­‰å¾…è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            time.sleep(3)
    
    print(f"âš ï¸ ç­‰å¾…è¶…æ—¶ ({max_wait_time}ç§’)ï¼Œä¿å­˜å½“å‰å†…å®¹")
    return False

def crawl_and_save_html(driver, domain, output_dir, max_wait_time=60):
    """çˆ¬å–å•ä¸ªç½‘ç«™å¹¶ç­‰å¾…å®Œå…¨åŠ è½½åä¿å­˜HTMLå†…å®¹å’Œæˆªå›¾ï¼ˆæ”¹è¿›ç‰ˆï¼‰"""
    try:
        # æ„å»ºURL
        url = f"http://{domain}"
        print(f"æ­£åœ¨è®¿é—®: {url}")
        
        # è®¾ç½®é¡µé¢åŠ è½½è¶…æ—¶
        driver.set_page_load_timeout(30)
        
        try:
            driver.get(url)
        except Exception as load_error:
            print(f"âŒ é¡µé¢åŠ è½½å¤±è´¥: {load_error}")
            return None
        
        # æ£€æŸ¥æ˜¯å¦æ˜¯é”™è¯¯é¡µé¢ï¼ˆGoDaddyå‡ºå”®é¡µé¢ç­‰ï¼‰
        title = driver.title.lower()
        body_text = driver.find_element(By.TAG_NAME, "body").text.lower()
        
        # æ£€æµ‹é”™è¯¯é¡µé¢å…³é”®è¯
        error_keywords = [
            'godaddy', 'domain', 'parked', 'for sale', 'buy this domain',
            'domain name', 'purchase', 'not found', '404', 'error',
        ]
        
        is_error_page = any(keyword in title or keyword in body_text for keyword in error_keywords)
        
        if is_error_page:
            print(f"âš ï¸ æ£€æµ‹åˆ°é”™è¯¯é¡µé¢ (æ ‡é¢˜: {driver.title})ï¼Œè·³è¿‡å¤„ç†")
            return None
        
        # ç­‰å¾…é¡µé¢å®Œå…¨åŠ è½½
        is_fully_loaded = wait_for_page_load(driver, max_wait_time)
        
        # é¢å¤–ç­‰å¾…ç¡®ä¿å†…å®¹å®Œå…¨æ¸²æŸ“
        if is_fully_loaded:
            print("é¢å¤–ç­‰å¾…5ç§’ç¡®ä¿å†…å®¹å®Œå…¨æ¸²æŸ“...")
            time.sleep(5)
        
        # è·å–é¡µé¢ä¿¡æ¯
        title = driver.title
        page_source = driver.page_source
        body_text = driver.find_element(By.TAG_NAME, "body").text
        
        # æ£€æŸ¥æ˜¯å¦è¿˜åœ¨loading
        loading_elements = driver.find_elements(By.ID, "preload")
        is_loading = len(loading_elements) > 0
        
        # ä¿å­˜HTMLæ–‡ä»¶åˆ°æŒ‡å®šç›®å½•
        html_filename = os.path.join(output_dir, f"{domain}.html")
        with open(html_filename, 'w', encoding='utf-8') as f:
            f.write(page_source)
        
        # ä¿å­˜æˆªå›¾åˆ°æŒ‡å®šç›®å½•ï¼ˆæ”¹è¿›ç‰ˆ - ä¿æŒçœŸå®æ¯”ä¾‹ï¼‰
        screenshot_filename = os.path.join(output_dir, f"{domain}.png")
        screenshot_saved = False
        
        try:
            print("å‡†å¤‡æˆªå›¾...")
            
            # è·å–é¡µé¢å®é™…å°ºå¯¸
            try:
                # è·å–é¡µé¢å®é™…å®½åº¦å’Œé«˜åº¦
                page_width = driver.execute_script("return document.documentElement.scrollWidth")
                page_height = driver.execute_script("return document.documentElement.scrollHeight")
                
                print(f"é¡µé¢å®é™…å°ºå¯¸: {page_width}x{page_height}px")
                
                # è®¡ç®—åˆé€‚çš„çª—å£å¤§å°ï¼Œä¿æŒçœŸå®æ¯”ä¾‹
                max_width = 1920
                max_height = 1080
                
                # å¦‚æœé¡µé¢å°ºå¯¸è¿‡å¤§ï¼ŒæŒ‰æ¯”ä¾‹ç¼©æ”¾
                if page_width > max_width or page_height > max_height:
                    scale = min(max_width / page_width, max_height / page_height)
                    window_width = int(page_width * scale)
                    window_height = int(page_height * scale)
                else:
                    window_width = page_width
                    window_height = page_height
                
                # ç¡®ä¿æœ€å°å°ºå¯¸
                window_width = max(window_width, 1200)
                window_height = max(window_height, 800)
                
                print(f"è®¾ç½®çª—å£å¤§å°: {window_width}x{window_height}px")
                
                # è®¾ç½®çª—å£å¤§å°
                driver.set_window_size(window_width, window_height)
                time.sleep(2)
                
                # æˆªå›¾
                driver.save_screenshot(screenshot_filename)
                print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜ (çœŸå®æ¯”ä¾‹): {screenshot_filename}")
                screenshot_saved = True
                
            except Exception as e1:
                print(f"æ–¹æ³•1å¤±è´¥: {e1}")
                
                # å¤‡ç”¨æ–¹æ³•ï¼šä½¿ç”¨å›ºå®šæ¯”ä¾‹
                try:
                    # ä½¿ç”¨16:9æ¯”ä¾‹
                    driver.set_window_size(1920, 1080)
                    time.sleep(2)
                    driver.save_screenshot(screenshot_filename)
                    print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜ (16:9æ¯”ä¾‹): {screenshot_filename}")
                    screenshot_saved = True
                    
                except Exception as e2:
                    print(f"æ–¹æ³•2ä¹Ÿå¤±è´¥: {e2}")
                    
                    # æ–¹æ³•3ï¼šæ»šåŠ¨æˆªå›¾
                    try:
                        driver.set_window_size(1920, 1080)
                        time.sleep(2)
                        
                        # æ»šåŠ¨åˆ°é¡µé¢åº•éƒ¨ç¡®ä¿å†…å®¹åŠ è½½
                        driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                        time.sleep(2)
                        driver.execute_script("window.scrollTo(0, 0);")
                        time.sleep(1)
                        
                        driver.save_screenshot(screenshot_filename)
                        print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜ (æ»šåŠ¨å): {screenshot_filename}")
                        screenshot_saved = True
                        
                    except Exception as e3:
                        print(f"æ‰€æœ‰æˆªå›¾æ–¹æ³•éƒ½å¤±è´¥: {e3}")
                        screenshot_saved = False
                        screenshot_filename = None
                        
        except Exception as screenshot_error:
            print(f"âš ï¸ æˆªå›¾å¤±è´¥: {screenshot_error}")
            screenshot_saved = False
            screenshot_filename = None
        
        result = {
            'url': url,
            'title': title,
            'is_loading': is_loading,
            'is_fully_loaded': is_fully_loaded,
            'body_text_length': len(body_text),
            'page_source_length': len(page_source),
            'html_file': html_filename,
            'screenshot_file': screenshot_filename,
            'screenshot_saved': screenshot_saved,
            'body_text_preview': body_text[:200] if body_text else "æ— æ–‡æœ¬å†…å®¹"
        }
        
        print(f"âœ“ çˆ¬å–å®Œæˆ: {title} (æ–‡æœ¬é•¿åº¦: {len(body_text)})")
        
        if is_fully_loaded and not is_loading:
            print(f"âœ… é¡µé¢å®Œå…¨åŠ è½½å®Œæˆå¹¶ä¿å­˜åˆ° {html_filename}")
        elif is_fully_loaded:
            print(f"âœ… é¡µé¢å†…å®¹å·²ç¨³å®šï¼Œä¿å­˜åˆ° {html_filename}")
        else:
            print(f"âš ï¸ é¡µé¢å¯èƒ½æœªå®Œå…¨åŠ è½½ï¼Œä½†å·²ä¿å­˜åˆ° {html_filename}")
        
        return result
        
    except Exception as e:
        print(f"çˆ¬å–å¤±è´¥ {domain}: {e}")
        return None

def batch_crawl_from_domains(domains, output_dir="C:/ml/results-combined-2", max_wait_time=60):
    """æ‰¹é‡çˆ¬å–ç½‘ç«™åˆ—è¡¨"""
    print(f"å¼€å§‹æ‰¹é‡çˆ¬å– {len(domains)} ä¸ªç½‘ç«™...")
    print(f"è¾“å‡ºç›®å½•: {output_dir}")
    
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(output_dir):
        os.makedirs(output_dir)
        print(f"âœ… åˆ›å»ºè¾“å‡ºç›®å½•: {output_dir}")
    
    # é…ç½®Chromeé€‰é¡¹
    chrome_options = Options()
    chrome_options.add_argument('--headless')  # æ— å¤´æ¨¡å¼
    chrome_options.add_argument('--no-sandbox')
    chrome_options.add_argument('--disable-dev-shm-usage')
    chrome_options.add_argument('--disable-gpu')
    chrome_options.add_argument('--window-size=1920,1080')
    chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36')
    
    try:
        # è‡ªåŠ¨å®‰è£…å¹¶å¯åŠ¨ChromeDriver
        print("æ­£åœ¨è‡ªåŠ¨å®‰è£…ChromeDriver...")
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        results = []
        success_count = 0
        skipped_count = 0
        
        for i, domain in enumerate(domains):
            print(f"\n=== è¿›åº¦: {i+1}/{len(domains)} ===")
            print(f"æ­£åœ¨å¤„ç†: {domain}")
            
            result = crawl_and_save_html(driver, domain, output_dir, max_wait_time)
            
            if result:
                results.append(result)
                success_count += 1
            else:
                skipped_count += 1
                print(f"â­ï¸ è·³è¿‡ {domain}")
            
            # é¿å…è¯·æ±‚è¿‡äºé¢‘ç¹
            if i < len(domains) - 1:  # ä¸æ˜¯æœ€åä¸€ä¸ª
                print("ç­‰å¾…3ç§’åç»§ç»­ä¸‹ä¸€ä¸ª...")
                time.sleep(3)
        
        # å…³é—­æµè§ˆå™¨
        driver.quit()
        
        # ç»Ÿè®¡æˆªå›¾æˆåŠŸæ•°é‡
        screenshot_success_count = sum(1 for result in results if result and result.get('screenshot_saved', False))
        
        # ä¿å­˜çˆ¬å–ç»“æœæ‘˜è¦
        summary_file = os.path.join(output_dir, "crawl_summary.json")
        summary = {
            'total_domains': len(domains),
            'success_count': success_count,
            'skipped_count': skipped_count,
            'failed_count': len(domains) - success_count - skipped_count,
            'success_rate': round(success_count / len(domains) * 100, 2),
            'screenshot_success_count': screenshot_success_count,
            'screenshot_success_rate': round(screenshot_success_count / len(domains) * 100, 2),
            'results': results
        }
        
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"\nğŸ‰ æ‰¹é‡çˆ¬å–å®Œæˆï¼")
        print(f"æ€»è®¡: {len(domains)} ä¸ªç½‘ç«™")
        print(f"æˆåŠŸ: {success_count} ä¸ª")
        print(f"è·³è¿‡: {skipped_count} ä¸ª (é”™è¯¯é¡µé¢/æ— æ³•è®¿é—®)")
        print(f"å¤±è´¥: {len(domains) - success_count - skipped_count} ä¸ª")
        print(f"æˆªå›¾æˆåŠŸ: {screenshot_success_count} ä¸ª")
        print(f"æˆåŠŸç‡: {summary['success_rate']}%")
        print(f"æˆªå›¾æˆåŠŸç‡: {summary['screenshot_success_rate']}%")
        print(f"ç»“æœæ‘˜è¦å·²ä¿å­˜åˆ°: {summary_file}")
        
        return summary
        
    except Exception as e:
        print(f"æ‰¹é‡çˆ¬å–å¤±è´¥: {e}")
        return None

def load_domains_from_file(filename="accessible_domains.json"):
    """ä»æ–‡ä»¶åŠ è½½åŸŸååˆ—è¡¨"""
    try:
        with open(filename, 'r', encoding='utf-8') as f:
            domains = json.load(f)
        print(f"âœ… ä» {filename} åŠ è½½äº† {len(domains)} ä¸ªåŸŸå")
        return domains
    except Exception as e:
        print(f"âŒ åŠ è½½åŸŸåæ–‡ä»¶å¤±è´¥: {e}")
        return []

if __name__ == "__main__":
    print("=== æ™ºèƒ½ç­‰å¾…åŠ è½½çˆ¬è™« ===")
    print("é€‰æ‹©è¿è¡Œæ¨¡å¼:")
    print("1. çˆ¬å–å•ä¸ªç½‘ç«™æµ‹è¯•")
    print("2. æ‰¹é‡çˆ¬å–æ‰€æœ‰åŸŸå")
    print("3. æ‰¹é‡çˆ¬å–å‰10ä¸ªåŸŸåï¼ˆæµ‹è¯•ï¼‰")
    
    choice = input("è¯·è¾“å…¥é€‰æ‹© (1, 2 æˆ– 3): ").strip()
    
    if choice == "1":
        # çˆ¬å–å•ä¸ªç½‘ç«™æµ‹è¯•
        test_domains = ["0x60df2k.cc"]
        batch_crawl_from_domains(test_domains, max_wait_time=45)
        
    elif choice == "2":
        # æ‰¹é‡çˆ¬å–æ‰€æœ‰åŸŸå
        domains = load_domains_from_file()
        if domains:
            batch_crawl_from_domains(domains, max_wait_time=30)
        else:
            print("æ— æ³•åŠ è½½åŸŸååˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ domains_list.json æ–‡ä»¶")
            
    elif choice == "3":
        # æ‰¹é‡çˆ¬å–å‰10ä¸ªåŸŸåï¼ˆæµ‹è¯•ï¼‰
        domains = load_domains_from_file()
        if domains:
            test_domains = domains[:10]
            print(f"æµ‹è¯•æ¨¡å¼ï¼šåªçˆ¬å–å‰ {len(test_domains)} ä¸ªåŸŸå")
            batch_crawl_from_domains(test_domains, max_wait_time=30)
        else:
            print("æ— æ³•åŠ è½½åŸŸååˆ—è¡¨ï¼Œè¯·æ£€æŸ¥ domains_list.json æ–‡ä»¶")
            
    else:
        print("æ— æ•ˆé€‰æ‹©")
